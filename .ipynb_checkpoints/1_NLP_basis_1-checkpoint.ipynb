{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1 NLP basis-1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNNv7XdPFp5To75fNwg6hrb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YQQXAamWTAE"
      },
      "outputs": [],
      "source": [
        "sample_text=\"These are short, famous texts in English from classic sources like the Bible or Shakespeare. Some texts have word definitions and explanations to help you. Some of these texts are written in an old style of English\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "DQBShfTnW0Sf",
        "outputId": "418cbf74-31b0-4041-f720-bad2611d0dd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'These are short, famous texts in English from classic sources like the Bible or Shakespeare. Some texts have word definitions and explanations to help you. Some of these texts are written in an old style of English'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should observe that the computer reads bodies of text, even if punctuated, as single string objects. Because of this, we need to find a way to separate this single body of text so that the computer evaluates each word as an individual\n",
        "\n",
        "This brings us to the concept of word tokenization, which is simply the process of separating a single string object, usually a body of text of varying length, into individual tokens that represent words or characters\n",
        "\n",
        "sample_text\n",
        "\n",
        "string object.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Natural Language Toolkit (NLTK) module.\n",
        "\n",
        "\n",
        "NLTK allows you to use some of the more basic NLP functionalities, as well as\n",
        "\n",
        "pretrained models for different tasks.\n",
        "\n",
        "It is my goal to allow you to train your own models, so we will not be working with any of the pretrained models in NLTK.\n",
        "\n",
        "However, you should read through the NLTK module documentation to become familiar with certain functions and algorithms that expedite text preprocessing.\n",
        "\n",
        "Relating back to our example, let's tokenize the sample data via the following code:"
      ],
      "metadata": {
        "id": "KwQrxUEUXt4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIMChqEVW6l-",
        "outputId": "6c9ba03e-fba8-4853-943b-06f72da9881a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentence Tokenize"
      ],
      "metadata": {
        "id": "JFpqjMICa_Hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sent_tokens = sent_tokenize(sample_text)"
      ],
      "metadata": {
        "id": "jhCI6pm8ZJea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample_sent_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiZTC7n0Znj2",
        "outputId": "2ea8648d-2d0c-4e60-ffab-75ed5d9490b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['These are short, famous texts in English from classic sources like the Bible or Shakespeare.', 'Some texts have word definitions and explanations to help you.', 'Some of these texts are written in an old style of English']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Word Tokenize()**"
      ],
      "metadata": {
        "id": "QAw3YqrNajJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_word_tokens = word_tokenize(sample_text)"
      ],
      "metadata": {
        "id": "rWP6Vh0BaAVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sample_word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8F0zcpPNaR-D",
        "outputId": "013c1745-94f0-4a8c-9dc8-2e9f94c3b330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['These', 'are', 'short', ',', 'famous', 'texts', 'in', 'English', 'from', 'classic', 'sources', 'like', 'the', 'Bible', 'or', 'Shakespeare', '.', 'Some', 'texts', 'have', 'word', 'definitions', 'and', 'explanations', 'to', 'help', 'you', '.', 'Some', 'of', 'these', 'texts', 'are', 'written', 'in', 'an', 'old', 'style', 'of', 'English']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# English StopWords"
      ],
      "metadata": {
        "id": "A6R8srPobINh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading stopwords Corpus"
      ],
      "metadata": {
        "id": "3xVUTEwlaVlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b7IXMRObQpA",
        "outputId": "e448e4b9-c672-4a6a-a0ef-803adfaaed63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Check the list of stop words"
      ],
      "metadata": {
        "id": "rQkA1fUbbVz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aMzCWpFbZRf",
        "outputId": "ad75c131-6aef-4410-a7c7-6926b2c58d7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords.fileids())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEihynqFbvkU",
        "outputId": "34ce7cc3-7c99-43e9-d940-290afd1914f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['arabic', 'azerbaijani', 'bengali', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Obntcax6cPyb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}